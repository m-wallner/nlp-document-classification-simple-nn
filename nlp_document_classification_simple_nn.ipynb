{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp-document-classification-simple-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wallner/nlp-document-classification-simple-nn/blob/main/nlp_document_classification_simple_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JphRNLIAsHdw"
      },
      "source": [
        "# Natural language processing: Document classification using a simple neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQfVagHA-PM"
      },
      "source": [
        "## 1 Imports and data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI7p8T4ZuljJ"
      },
      "source": [
        "!pip install torchtext==0.8.1\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugzr5CnpyOuk"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn  import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "from torchtext.data import Field, Dataset, Example, BucketIterator\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQPj72zvygFr",
        "outputId": "952fe2b8-e5c4-431c-f8cd-94e757d46708"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define data paths\n",
        "data_path = '/content/gdrive/My Drive/Colab Notebooks/data/NLP/A1'\n",
        "labels_path = '/content/gdrive/My Drive/Colab Notebooks/data/NLP/A1/thedeep/thedeep.labels.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1NWgjtmtbHz"
      },
      "source": [
        "### 1.1 Loading thedeep dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "wx9rWjExyhyG",
        "outputId": "7102c4c7-f6f6-41f0-c2ba-403aee0bdcb0"
      },
      "source": [
        "# Load thedeep training dataset into Pandas dataframe\n",
        "thedeep_df_train = pd.read_csv(\n",
        "  os.path.join(data_path, 'thedeep/thedeep.medium.train.txt'),\n",
        "  sep=',',\n",
        "  names=['sentence_id', 'text', 'label'],\n",
        "  index_col=0,\n",
        "  skiprows=[0]\n",
        ")\n",
        "\n",
        "# Load thedeep validation dataset into Pandas dataframe\n",
        "thedeep_df_valid = pd.read_csv(\n",
        "  os.path.join(data_path, 'thedeep/thedeep.medium.validation.txt'),\n",
        "  sep=',',\n",
        "  names=['sentence_id', 'text', 'label'],\n",
        "  index_col=0,\n",
        "  skiprows=[0]\n",
        ")\n",
        "\n",
        "# Load thedeep test dataset into Pandas dataframe\n",
        "thedeep_df_test = pd.read_csv(\n",
        "  os.path.join(data_path, 'thedeep/thedeep.medium.test.txt'),\n",
        "  sep=',',\n",
        "  names=['sentence_id', 'text', 'label'],\n",
        "  index_col=0,\n",
        "  skiprows=[0]\n",
        ")\n",
        "\n",
        "# Show structure of thedeep dataset\n",
        "thedeep_df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentence_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28291</th>\n",
              "      <td>The primary reported needs for IDPs across the...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9695</th>\n",
              "      <td>Some 602 000 IDPs are now spread across the co...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7781</th>\n",
              "      <td>South Sudanese soldiers accused of raping at l...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31382</th>\n",
              "      <td>Since the beginning of 2017, 18 882 suspected/...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19919</th>\n",
              "      <td>The number of new suspected cholera cases in 2...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          text  label\n",
              "sentence_id                                                          \n",
              "28291        The primary reported needs for IDPs across the...      4\n",
              "9695         Some 602 000 IDPs are now spread across the co...      3\n",
              "7781         South Sudanese soldiers accused of raping at l...      9\n",
              "31382        Since the beginning of 2017, 18 882 suspected/...     11\n",
              "19919        The number of new suspected cholera cases in 2...     11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOmRIY3h2tGE"
      },
      "source": [
        "### 1.2 Basic information about thedeep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlzcX5mN3Oyo",
        "outputId": "9e31ad08-ce0e-4173-823a-e594795f63d8"
      },
      "source": [
        "# Load label captions\n",
        "labelcaptions = {}\n",
        "with open(labels_path) as fr:\n",
        "  for label in fr:\n",
        "    vals = label.strip().split(',')\n",
        "    labelcaptions[vals[1]] = int(vals[0])\n",
        "    \n",
        "# Show labels and corresponding numbers\n",
        "labelcaptions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Agriculture': 0,\n",
              " 'Cross': 1,\n",
              " 'Education': 2,\n",
              " 'Food': 3,\n",
              " 'Health': 4,\n",
              " 'Livelihood': 5,\n",
              " 'Logistic': 6,\n",
              " 'NFI': 7,\n",
              " 'Nutrition': 8,\n",
              " 'Protection': 9,\n",
              " 'Shelter': 10,\n",
              " 'WASH': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0KbzpvP3gOC",
        "outputId": "58d344ad-2803-437f-d216-2e8f589a35f8"
      },
      "source": [
        "# Show number of training samples per label\n",
        "thedeep_df_train['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     5419\n",
              "9     4618\n",
              "3     4341\n",
              "10    2553\n",
              "11    2178\n",
              "5     1712\n",
              "2     1278\n",
              "8     1207\n",
              "1     1066\n",
              "7     1054\n",
              "0      743\n",
              "6      430\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN4Nitt5X6dO",
        "outputId": "3703684c-bc5b-4331-d295-17ab25268aa7"
      },
      "source": [
        "# Show number of validation samples per label\n",
        "thedeep_df_valid['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     1196\n",
              "9      960\n",
              "3      954\n",
              "10     474\n",
              "11     463\n",
              "5      378\n",
              "2      300\n",
              "8      264\n",
              "1      232\n",
              "7      229\n",
              "0      168\n",
              "6       81\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOF15Wi8X64M",
        "outputId": "88158fb5-65f6-4b35-a9a7-6bd1e8e3e67f"
      },
      "source": [
        "# Show number of test samples per label\n",
        "thedeep_df_test['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     1181\n",
              "9      957\n",
              "3      944\n",
              "10     509\n",
              "11     484\n",
              "5      382\n",
              "2      283\n",
              "8      272\n",
              "1      223\n",
              "7      193\n",
              "0      177\n",
              "6       94\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34rlhNcTBWV7"
      },
      "source": [
        "## 2 Data preprocessing, word embedding and saving\n",
        "\n",
        "Just executed once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv0UcnO6up5l"
      },
      "source": [
        "### 2.1 Define torchtext.Field and apply preprocessing steps to thedeep dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv3pR8JA2cqA",
        "outputId": "949bec03-4f1f-4a87-a2ff-3f3f429f822a"
      },
      "source": [
        "# Define torchtext.Field objects for Tensor representation of data\n",
        "text_field = Field(tokenize='spacy', lower=True, batch_first=True)\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
        "fields = [('')]\n",
        "\n",
        "# Apply preprocessing to training, validation and test set\n",
        "text_train_pre = thedeep_df_train['text'].apply(lambda x: text_field.preprocess(x))\n",
        "text_valid_pre = thedeep_df_valid['text'].apply(lambda x: text_field.preprocess(x))\n",
        "text_test_pre = thedeep_df_test['text'].apply(lambda x: text_field.preprocess(x))\n",
        "\n",
        "text_train_pre"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentence_id\n",
              "28291    [the, primary, reported, needs, for, idps, acr...\n",
              "9695     [some, 602,  , 000, idps, are, now, spread, ac...\n",
              "7781     [south, sudanese, soldiers, accused, of, rapin...\n",
              "31382    [since, the, beginning, of, 2017, ,, 18, 882, ...\n",
              "19919    [the, number, of, new, suspected, cholera, cas...\n",
              "                               ...                        \n",
              "36292    [cholera, continues, to, spread, in, yemen, ,,...\n",
              "5566     [an, estimated, 165,000, children, are, expect...\n",
              "19676    [on, 3, march, 2017, ,, tropical, storm, enawo...\n",
              "29831    [the, presence, of, uxo, was, reported, in, 15...\n",
              "27747    [as, at, week, 27, (, july, 1, -, 7, ,, 2017, ...\n",
              "Name: text, Length: 26599, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFb3tzVY3E10"
      },
      "source": [
        "### 2.2 Load GloVe.6B.300d word embeddings, create dictionary and word embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaiMnXvXP65P",
        "outputId": "0eb6a7fd-f57e-4070-da01-22438b1a681f"
      },
      "source": [
        "# Load GloVe6B.300d word embedding - takes a LOOONG time - and build\n",
        "# GloVe-based vocabulary for all datasets\n",
        "text_field.build_vocab(text_train_pre, vectors='glove.6B.300d')\n",
        "text_field.build_vocab(text_valid_pre, vectors='glove.6B.300d')\n",
        "text_field.build_vocab(text_test_pre, vectors='glove.6B.300d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399998/400000 [00:37<00:00, 10858.42it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x-OnxuoZaB4",
        "outputId": "9ca1c8fc-db67-44ac-bb68-7b3b15654df5"
      },
      "source": [
        "# Checking total number of different words in corpus after preprocessing\n",
        "text_pre = [text_train_pre, text_valid_pre, text_test_pre]\n",
        "dictionary = {}\n",
        "for text in text_pre:\n",
        "  for doc in text:\n",
        "    for word in doc:\n",
        "      if word not in dictionary: dictionary[word] = 1\n",
        "      else: dictionary[word] += 1\n",
        "\n",
        "print(f'Length of dictionary: {len(dictionary)} words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of dictionary: 48817 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C2rE80Vu1yk"
      },
      "source": [
        "### 2.3 Initialize words not found in vocabulary with random values from a normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqP4yzs0N9Oc",
        "outputId": "84e38c38-a7ae-4d65-f049-b762268d7215"
      },
      "source": [
        "# Get torchtext.vocab instance and show the structure of the tensor.\n",
        "# The whole corpus is in one big tensor.\n",
        "text_field.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVu7axT5KSh_",
        "outputId": "43a3f61b-b661-4694-ee73-d3366aaf7159"
      },
      "source": [
        "# Turn all words which were not contained in Glove vocabulary from zero vectors into random\n",
        "# vectors with a normal distribution\n",
        "\n",
        "# Define zero vector to compare other vectors to\n",
        "zero_tensor = torch.zeros_like(text_field.vocab.vectors[0])\n",
        "\n",
        "# Turn zero vectors in vocabulary.vectors to random vectors with std = 1\n",
        "counter = 0\n",
        "for i, vector in enumerate(text_field.vocab.vectors):\n",
        "  if torch.all(torch.eq(vector, zero_tensor)):\n",
        "    text_field.vocab.vectors[i] = torch.randn_like(zero_tensor)\n",
        "    counter += 1\n",
        "\n",
        "print(f'{counter} new words initialized randomly with normally distributed values \\n')\n",
        "\n",
        "# Show updated tensor without zero-vectors\n",
        "text_field.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4538 new words initialized randomly with normally distributed values \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3601, -0.4453,  1.6286,  ..., -0.9567,  0.9818,  0.1765],\n",
              "        [-2.4346, -0.3123, -0.9448,  ..., -2.1006, -0.7532,  0.8999],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 1.0273,  0.0028, -0.3037,  ..., -2.4012, -0.5784, -0.6563],\n",
              "        [ 0.3459,  0.4757,  0.1960,  ...,  0.8434,  2.1771,  0.0535],\n",
              "        [-0.3181, -1.0090,  0.6965,  ..., -0.9082,  0.0988, -1.5894]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1LIzlzAsySE"
      },
      "source": [
        "### 2.4 Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDQPlFglN_hm"
      },
      "source": [
        "# Pickle preprocessed and embedded data\n",
        "with open(os.path.join(data_path, 'text_field.pickle'), 'wb') as f:\n",
        "    pickle.dump(text_field, f)\n",
        "with open(os.path.join(data_path, 'label_field.pickle'), 'wb') as f:\n",
        "    pickle.dump(label_field, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0xWX0nEvSfM"
      },
      "source": [
        "## 3 Load preprocessed and embedded data and construct Dataset object from pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXrElSQNkFIm",
        "outputId": "d8b80ef9-4749-47b9-efe3-e6300240be72"
      },
      "source": [
        "# Load preprocessed and embedded data\n",
        "with open(os.path.join(data_path, 'text_field.pickle'), 'rb') as f:\n",
        "    text_field = pickle.load(f)\n",
        "with open(os.path.join(data_path, 'label_field.pickle'), 'rb') as f:\n",
        "    label_field = pickle.load(f)\n",
        "\n",
        "# Define torchtext Dataset class to load pandas DataFrame\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df:pd.DataFrame, fields:list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [Example.fromlist(list(r), fields) for i, r in df.iterrows()], fields\n",
        "        )\n",
        "\n",
        "# Construct DataFrameDataset for all datasets\n",
        "fields = (('text', text_field), ('label', label_field))\n",
        "train_dataset = DataFrameDataset(df=thedeep_df_train, fields=fields)\n",
        "valid_dataset = DataFrameDataset(df=thedeep_df_valid, fields=fields)\n",
        "test_dataset = DataFrameDataset(df=thedeep_df_test, fields=fields)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxJOH8-TfNMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff19fa82-356d-457c-83d5-e25a7956bccf"
      },
      "source": [
        "# Example sentence in torchtext.data.Example object\n",
        "train_dataset[0].text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'primary',\n",
              " 'reported',\n",
              " 'needs',\n",
              " 'for',\n",
              " 'idps',\n",
              " 'across',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'of',\n",
              " 'libya',\n",
              " 'were',\n",
              " 'access',\n",
              " 'to',\n",
              " 'food',\n",
              " ',',\n",
              " 'health',\n",
              " 'services',\n",
              " 'and',\n",
              " 'shelter',\n",
              " '.',\n",
              " 'the',\n",
              " 'main',\n",
              " 'issues',\n",
              " 'related',\n",
              " 'to',\n",
              " 'the',\n",
              " 'above',\n",
              " '-',\n",
              " 'mentioned',\n",
              " 'needs',\n",
              " 'are',\n",
              " 'that',\n",
              " 'goods',\n",
              " 'are',\n",
              " 'too',\n",
              " 'expensive',\n",
              " 'and',\n",
              " 'therefore',\n",
              " 'idps',\n",
              " 'have',\n",
              " 'limit',\n",
              " 'access',\n",
              " '.',\n",
              " 'other',\n",
              " 'issues',\n",
              " 'cited',\n",
              " 'for',\n",
              " 'access',\n",
              " 'to',\n",
              " 'health',\n",
              " 'included',\n",
              " 'irregular',\n",
              " 'supply',\n",
              " 'of',\n",
              " 'medicines',\n",
              " 'and',\n",
              " 'low',\n",
              " 'quality',\n",
              " 'of',\n",
              " 'available',\n",
              " 'health',\n",
              " 'services',\n",
              " 'due',\n",
              " 'to',\n",
              " 'overcrowded',\n",
              " 'facilities',\n",
              " ',',\n",
              " 'lack',\n",
              " 'of',\n",
              " 'medical',\n",
              " 'staff',\n",
              " 'and',\n",
              " 'a',\n",
              " 'diminished',\n",
              " 'availability',\n",
              " 'of',\n",
              " 'female',\n",
              " 'doctors',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBNxhJa9B8Qm"
      },
      "source": [
        "## 4 Definition of a simple neural network and training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP-JNQv1wvvB"
      },
      "source": [
        "### 4.1 Simple fully connected model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zx897OWyS_A"
      },
      "source": [
        "# Define model architecture\n",
        "class ClassificationAverageModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "    super().__init__()\n",
        "    # Calculate document representation as mean of word vectors\n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode='mean')\n",
        "    self.fc = nn.Linear(embed_dim, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  # Initiating weights method\n",
        "  def init_weights(self):\n",
        "    initrange = 1\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.bias.data.zero_()\n",
        "\n",
        "  # Define forward method\n",
        "  def forward(self, text, offsets):\n",
        "    embedded = self.embedding(text, offsets)\n",
        "    return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz_WIMrewmsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d119cd70-9f4f-4314-a171-2a77da97b043"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(text_field.vocab.vectors)\n",
        "EMBED_DIM = 300\n",
        "N_CLASSES = len(labelcaptions)\n",
        "N_EPOCHS = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gTAQR_C4pjf"
      },
      "source": [
        "### 4.2 collate_fn function for PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHDpWf_vdgZd"
      },
      "source": [
        "# Data batching\n",
        "# Text entries have different lengths => use custom function to generate data\n",
        "# batches and offsets, then pass it to collate_fn in Pytorch DataLoader\n",
        "\n",
        "# Get vocabulary from text_field\n",
        "vocabulary = text_field.vocab.stoi\n",
        "\n",
        "def generate_batch(batch):\n",
        "  label = torch.tensor([int(entry.label) for entry in batch])\n",
        "  text = []\n",
        "  for sample in batch:\n",
        "    sample_list = [vocabulary[word] for word in sample.text]\n",
        "    text.append(torch.LongTensor(sample_list))\n",
        "\n",
        "  offsets = [0] + [len(entry) for entry in text]\n",
        "\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text = torch.cat(text)\n",
        "\n",
        "  return text, offsets, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsntOC4a4zVV"
      },
      "source": [
        "### 4.3 Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nciziej-GiKM"
      },
      "source": [
        "# Define an instance of the model class\n",
        "model = ClassificationAverageModel(VOCAB_SIZE, EMBED_DIM, N_CLASSES).to(device)\n",
        "\n",
        "# Get vocabulary from text_field\n",
        "vocabulary = text_field.vocab.stoi\n",
        "\n",
        "def train(sub_train_):\n",
        "  train_loss, train_acc = 0, 0\n",
        "  data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "  for i, (text, offsets, cls) in enumerate(data):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Offsets are necessary since the tensor consists of the mean vectors of\n",
        "    # every document - offsets point to the end of one document vector.\n",
        "    text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "    output = model(text, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  # Adjust learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "\n",
        "def test(data_):\n",
        "  loss, acc = 0, 0\n",
        "  data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "\n",
        "  # Offsets are necessary since the tensor consists of the mean vectors of\n",
        "  # every document - offsets point to the end of one document vector.\n",
        "  for text, offsets, cls in data:\n",
        "    text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "    with torch.no_grad(): # we do not want to update any weights here\n",
        "      output = model(text, offsets)\n",
        "      loss = criterion(output, cls)\n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gy0ZcKbU4E"
      },
      "source": [
        "## 5 Model training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7IxqyIaErED"
      },
      "source": [
        "Cross Entropy Loss used instead of Negative Log Likelihood Loss, in order to save an additional layer, which would have been otherwise necessary. See PyTorch documentation: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roJLbCBdLKpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172f4e42-f738-4911-bbc3-06f312202f21"
      },
      "source": [
        "# Split dataset and run model\n",
        "\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "# Set Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(list(model.parameters()), lr=4.0) # Extremely high lr for testing purposes\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "# Get validation set from the training set\n",
        "train_len = int(len(train_dataset) * 0.90)\n",
        "sub_train_, sub_valid_ = \\\n",
        "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
        "\n",
        "# Implement early stopping\n",
        "best_val_loss = 10000\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    # stop time, accuracies and losses\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(train_dataset)\n",
        "    valid_loss, valid_acc = test(valid_dataset)\n",
        "    \n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
        "\n",
        "    # early stopping\n",
        "    if valid_loss < best_val_loss:\n",
        "      with open('model.pt', 'wb') as f:\n",
        "        torch.save(model, f) # save current state of model\n",
        "        best_val_loss = valid_loss\n",
        "        counter = 0 # reset counter if a new best validation loss was found\n",
        "\n",
        "    else: # if counter reaches 5, we exit the training loop\n",
        "      counter += 1\n",
        "    \n",
        "    if counter == 5:\n",
        "      # break for loop and stop training\n",
        "      print(f'\\tEarly stopping triggered - model stopped training process after epoch: {epoch + 1}')\n",
        "      break\n",
        "\n",
        "# after training the model and the saving of the best performing model\n",
        "# we load the best performing model and evaluate it on the test set\n",
        "with open('model.pt', 'rb') as f:\n",
        "  model = torch.load(f)\n",
        "\n",
        "test_loss, test_acc = test(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 31.9914(train)\t|\tAcc: 46.9%(train)\n",
            "\tLoss: 0.5146(valid)\t|\tAcc: 49.4%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 48.4257(train)\t|\tAcc: 57.6%(train)\n",
            "\tLoss: 0.3751(valid)\t|\tAcc: 55.2%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 44.0875(train)\t|\tAcc: 63.4%(train)\n",
            "\tLoss: 0.0919(valid)\t|\tAcc: 55.1%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 35.3688(train)\t|\tAcc: 67.1%(train)\n",
            "\tLoss: 1.2207(valid)\t|\tAcc: 56.1%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 30.3734(train)\t|\tAcc: 69.5%(train)\n",
            "\tLoss: 0.3380(valid)\t|\tAcc: 57.2%(valid)\n",
            "Epoch: 6  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 25.1339(train)\t|\tAcc: 71.8%(train)\n",
            "\tLoss: 1.0767(valid)\t|\tAcc: 57.2%(valid)\n",
            "Epoch: 7  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 21.4813(train)\t|\tAcc: 73.2%(train)\n",
            "\tLoss: 0.0354(valid)\t|\tAcc: 57.2%(valid)\n",
            "Epoch: 8  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 19.0923(train)\t|\tAcc: 74.2%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 57.5%(valid)\n",
            "Epoch: 9  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 16.1157(train)\t|\tAcc: 75.8%(train)\n",
            "\tLoss: 0.5045(valid)\t|\tAcc: 58.6%(valid)\n",
            "Epoch: 10  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 14.3544(train)\t|\tAcc: 76.0%(train)\n",
            "\tLoss: 0.1912(valid)\t|\tAcc: 58.7%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S75vz8EBi8tW"
      },
      "source": [
        "**Main sources used:**\n",
        "\n",
        "Pytorch doc - EmbeddingBag: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "\n",
        "Glove and Pytorch: https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f"
      ]
    }
  ]
}